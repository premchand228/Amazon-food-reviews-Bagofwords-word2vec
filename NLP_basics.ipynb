{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2d20ebf",
   "metadata": {},
   "source": [
    "## NLP sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e08241",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "357416aa",
   "metadata": {},
   "source": [
    "- 1) Lets say you have amazon product reviews and you want to develop a model that will classify the review as either +ve or -ve.\n",
    "\n",
    "- 2) Now our task is to build a model that will separate our +ve and -ve reviews using a hyper plane so that all my positive reviews are one side of plane and negitive reviews are     another side of the plane \n",
    "\n",
    "- Given any problem if we can convert the input as vectors we can leverage the power of linera algebra . \n",
    "\n",
    "- Big question is how do we convert simple english text into mathmatical/numerical vectors.\n",
    "\n",
    "\n",
    "- One solution could be bag of words.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07732469",
   "metadata": {},
   "source": [
    "## Bag of words code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d27cd05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"netflix.csv\")[\"description\"][0:100]\n",
    "\n",
    "uniquewords=set(\"\".join(df.unique()).split(\" \"))\n",
    "len(uniquewords)\n",
    "df1=pd.DataFrame(np.zeros((df.shape[0],len(uniquewords))))\n",
    "df1.columns=uniquewords\n",
    "\n",
    "for i in df:\n",
    "    #print(i)\n",
    "    for j in i.split(\" \"):\n",
    "        df1[j]=i.count(j)\n",
    "for i in df1.columns:\n",
    "    df1[i]=df1[i].astype(\"int\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "838e8c6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b8f2c702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Weston</th>\n",
       "      <th>uptight</th>\n",
       "      <th>forward.Sicily</th>\n",
       "      <th>Otto</th>\n",
       "      <th>terrible</th>\n",
       "      <th>married</th>\n",
       "      <th>sled</th>\n",
       "      <th>detective.On</th>\n",
       "      <th>cosmic</th>\n",
       "      <th>It's</th>\n",
       "      <th>...</th>\n",
       "      <th>below.</th>\n",
       "      <th>Elisabeth</th>\n",
       "      <th>barriers?</th>\n",
       "      <th>sale.</th>\n",
       "      <th>Status</th>\n",
       "      <th>$100,000.</th>\n",
       "      <th>Using</th>\n",
       "      <th>Zant.</th>\n",
       "      <th>alone.</th>\n",
       "      <th>risk.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 1413 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Weston  uptight  forward.Sicily  Otto  terrible  married  sled  \\\n",
       "0        1        1               0     1         1        1     1   \n",
       "1        1        1               0     1         1        1     1   \n",
       "2        1        1               0     1         1        1     1   \n",
       "3        1        1               0     1         1        1     1   \n",
       "4        1        1               0     1         1        1     1   \n",
       "..     ...      ...             ...   ...       ...      ...   ...   \n",
       "95       1        1               0     1         1        1     1   \n",
       "96       1        1               0     1         1        1     1   \n",
       "97       1        1               0     1         1        1     1   \n",
       "98       1        1               0     1         1        1     1   \n",
       "99       1        1               0     1         1        1     1   \n",
       "\n",
       "    detective.On  cosmic  It's  ...  below.  Elisabeth  barriers?  sale.  \\\n",
       "0              0       1     1  ...       1          1          1      1   \n",
       "1              0       1     1  ...       1          1          1      1   \n",
       "2              0       1     1  ...       1          1          1      1   \n",
       "3              0       1     1  ...       1          1          1      1   \n",
       "4              0       1     1  ...       1          1          1      1   \n",
       "..           ...     ...   ...  ...     ...        ...        ...    ...   \n",
       "95             0       1     1  ...       1          1          1      1   \n",
       "96             0       1     1  ...       1          1          1      1   \n",
       "97             0       1     1  ...       1          1          1      1   \n",
       "98             0       1     1  ...       1          1          1      1   \n",
       "99             0       1     1  ...       1          1          1      1   \n",
       "\n",
       "    Status  $100,000.  Using  Zant.  alone.  risk.  \n",
       "0        1          1      1      1       1      1  \n",
       "1        1          1      1      1       1      1  \n",
       "2        1          1      1      1       1      1  \n",
       "3        1          1      1      1       1      1  \n",
       "4        1          1      1      1       1      1  \n",
       "..     ...        ...    ...    ...     ...    ...  \n",
       "95       1          1      1      1       1      1  \n",
       "96       1          1      1      1       1      1  \n",
       "97       1          1      1      1       1      1  \n",
       "98       1          1      1      1       1      1  \n",
       "99       1          1      1      1       1      1  \n",
       "\n",
       "[100 rows x 1413 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5dd8350b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As\n",
      "her\n",
      "father\n",
      "nears\n",
      "the\n",
      "end\n",
      "of\n",
      "his\n",
      "life,\n",
      "filmmaker\n",
      "Kirsten\n",
      "Johnson\n",
      "stages\n",
      "his\n",
      "i'm inside\n",
      "death\n",
      "in\n",
      "inventive\n",
      "and\n",
      "comical\n",
      "ways\n",
      "to\n",
      "help\n",
      "them\n",
      "both\n",
      "face\n",
      "the\n",
      "i'm inside\n",
      "inevitable.\n",
      "After\n",
      "crossing\n",
      "paths\n",
      "at\n",
      "a\n",
      "party,\n",
      "a\n",
      "i'm inside\n",
      "Cape\n",
      "Town\n",
      "teen\n",
      "sets\n",
      "out\n",
      "to\n",
      "i'm inside\n",
      "prove\n",
      "whether\n",
      "a\n",
      "i'm inside\n",
      "private-school\n",
      "swimming\n",
      "star\n",
      "is\n",
      "her\n",
      "i'm inside\n",
      "sister\n",
      "who\n",
      "was\n",
      "abducted\n",
      "at\n",
      "i'm inside\n",
      "birth.\n",
      "To\n",
      "protect\n",
      "his\n",
      "i'm inside\n",
      "family\n",
      "from\n",
      "a\n",
      "i'm inside\n",
      "powerful\n",
      "drug\n",
      "lord,\n",
      "skilled\n",
      "thief\n",
      "Mehdi\n",
      "and\n",
      "i'm inside\n",
      "his\n",
      "i'm inside\n",
      "expert\n",
      "team\n",
      "of\n",
      "i'm inside\n",
      "robbers\n",
      "are\n",
      "pulled\n",
      "into\n",
      "a\n",
      "i'm inside\n",
      "violent\n",
      "and\n",
      "i'm inside\n",
      "deadly\n",
      "turf\n",
      "war.\n",
      "Feuds,\n",
      "flirtations\n",
      "and\n",
      "i'm inside\n",
      "toilet\n",
      "talk\n",
      "go\n",
      "down\n",
      "among\n",
      "the\n",
      "i'm inside\n",
      "incarcerated\n",
      "women\n",
      "at\n",
      "i'm inside\n",
      "the\n",
      "i'm inside\n",
      "Orleans\n",
      "Justice\n",
      "Center\n",
      "in\n",
      "i'm inside\n",
      "New\n",
      "Orleans\n",
      "i'm inside\n",
      "on\n",
      "this\n",
      "gritty\n",
      "reality\n",
      "series.\n",
      "In\n",
      "a\n",
      "i'm inside\n",
      "city\n",
      "of\n",
      "i'm inside\n",
      "coaching\n",
      "centers\n",
      "known\n",
      "to\n",
      "i'm inside\n",
      "train\n",
      "Indiaâ€™s\n",
      "finest\n",
      "collegiate\n",
      "minds,\n",
      "an\n",
      "earnest\n",
      "but\n",
      "unexceptional\n",
      "student\n",
      "and\n",
      "i'm inside\n",
      "his\n",
      "i'm inside\n",
      "friends\n",
      "navigate\n",
      "campus\n",
      "life.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "dict1={}\n",
    "for i in df.head(5):\n",
    "    for  j in i.split(\" \"):\n",
    "        print(j)\n",
    "        if j  in dict1.keys():\n",
    "            print(\"i'm inside\")\n",
    "            dict1[j]=dict1.get(j)+1\n",
    "        else:\n",
    "            dict1[j]=1\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8433ee3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['As', 'her', 'father', 'nears', 'the', 'end', 'of', 'his', 'life,', 'filmmaker', 'Kirsten', 'Johnson', 'stages', 'death', 'in', 'inventive', 'and', 'comical', 'ways', 'to', 'help', 'them', 'both', 'face', 'inevitable.', 'After', 'crossing', 'paths', 'at', 'a', 'party,', 'Cape', 'Town', 'teen', 'sets', 'out', 'prove', 'whether', 'private-school', 'swimming', 'star', 'is', 'sister', 'who', 'was', 'abducted', 'birth.', 'To', 'protect', 'family', 'from', 'powerful', 'drug', 'lord,', 'skilled', 'thief', 'Mehdi', 'expert', 'team', 'robbers', 'are', 'pulled', 'into', 'violent', 'deadly', 'turf', 'war.', 'Feuds,', 'flirtations', 'toilet', 'talk', 'go', 'down', 'among', 'incarcerated', 'women', 'Orleans', 'Justice', 'Center', 'New', 'on', 'this', 'gritty', 'reality', 'series.', 'In', 'city', 'coaching', 'centers', 'known', 'train', 'Indiaâ€™s', 'finest', 'collegiate', 'minds,', 'an', 'earnest', 'but', 'unexceptional', 'student', 'friends', 'navigate', 'campus', 'life.'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b15f275f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'As': 1,\n",
       " 'her': 2,\n",
       " 'father': 1,\n",
       " 'nears': 1,\n",
       " 'the': 4,\n",
       " 'end': 1,\n",
       " 'of': 3,\n",
       " 'his': 5,\n",
       " 'life,': 1,\n",
       " 'filmmaker': 1,\n",
       " 'Kirsten': 1,\n",
       " 'Johnson': 1,\n",
       " 'stages': 1,\n",
       " 'death': 1,\n",
       " 'in': 2,\n",
       " 'inventive': 1,\n",
       " 'and': 5,\n",
       " 'comical': 1,\n",
       " 'ways': 1,\n",
       " 'to': 3,\n",
       " 'help': 1,\n",
       " 'them': 1,\n",
       " 'both': 1,\n",
       " 'face': 1,\n",
       " 'inevitable.': 1,\n",
       " 'After': 1,\n",
       " 'crossing': 1,\n",
       " 'paths': 1,\n",
       " 'at': 3,\n",
       " 'a': 6,\n",
       " 'party,': 1,\n",
       " 'Cape': 1,\n",
       " 'Town': 1,\n",
       " 'teen': 1,\n",
       " 'sets': 1,\n",
       " 'out': 1,\n",
       " 'prove': 1,\n",
       " 'whether': 1,\n",
       " 'private-school': 1,\n",
       " 'swimming': 1,\n",
       " 'star': 1,\n",
       " 'is': 1,\n",
       " 'sister': 1,\n",
       " 'who': 1,\n",
       " 'was': 1,\n",
       " 'abducted': 1,\n",
       " 'birth.': 1,\n",
       " 'To': 1,\n",
       " 'protect': 1,\n",
       " 'family': 1,\n",
       " 'from': 1,\n",
       " 'powerful': 1,\n",
       " 'drug': 1,\n",
       " 'lord,': 1,\n",
       " 'skilled': 1,\n",
       " 'thief': 1,\n",
       " 'Mehdi': 1,\n",
       " 'expert': 1,\n",
       " 'team': 1,\n",
       " 'robbers': 1,\n",
       " 'are': 1,\n",
       " 'pulled': 1,\n",
       " 'into': 1,\n",
       " 'violent': 1,\n",
       " 'deadly': 1,\n",
       " 'turf': 1,\n",
       " 'war.': 1,\n",
       " 'Feuds,': 1,\n",
       " 'flirtations': 1,\n",
       " 'toilet': 1,\n",
       " 'talk': 1,\n",
       " 'go': 1,\n",
       " 'down': 1,\n",
       " 'among': 1,\n",
       " 'incarcerated': 1,\n",
       " 'women': 1,\n",
       " 'Orleans': 2,\n",
       " 'Justice': 1,\n",
       " 'Center': 1,\n",
       " 'New': 1,\n",
       " 'on': 1,\n",
       " 'this': 1,\n",
       " 'gritty': 1,\n",
       " 'reality': 1,\n",
       " 'series.': 1,\n",
       " 'In': 1,\n",
       " 'city': 1,\n",
       " 'coaching': 1,\n",
       " 'centers': 1,\n",
       " 'known': 1,\n",
       " 'train': 1,\n",
       " 'Indiaâ€™s': 1,\n",
       " 'finest': 1,\n",
       " 'collegiate': 1,\n",
       " 'minds,': 1,\n",
       " 'an': 1,\n",
       " 'earnest': 1,\n",
       " 'but': 1,\n",
       " 'unexceptional': 1,\n",
       " 'student': 1,\n",
       " 'friends': 1,\n",
       " 'navigate': 1,\n",
       " 'campus': 1,\n",
       " 'life.': 1}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1518521a",
   "metadata": {},
   "source": [
    "## Important notes about Bag of words \n",
    "\n",
    "- Lets say we have r1 ,r2 and r3 sentences and if r1 and r2 are similar then lenght of vector1-vector2 will be lesser than length of vecto3-vector1\n",
    "- Binary nag of words are nothing but identifying no of words that are different . Lets say if there are 9 words in corpus and 6 aare similar and \n",
    "- 3 words are different then you get v1-v2 as square root of 3. when there are high no of words athat are different from one vector to other then  those two vectors are dissimilar. \n",
    "\n",
    "- Generally all positive polarity reviews will have less distance from one vector to other and distance  between positive class and negitive class will be high \n",
    "\n",
    "- Bagwords extract common  words \n",
    "\n",
    "- Bag of words count no of occurences where as binary bag of words instead of using count of words it just use 1 or 0 to make an understanding if word exists or not.\n",
    "\n",
    "- the difference between v1 and v2 is no of differing words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a3cc34",
   "metadata": {},
   "source": [
    "# NLP Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebe560f",
   "metadata": {},
   "source": [
    "- Removing stops words\n",
    "\n",
    "Generally, English stop words does not add any meaning to sentence rather they will increase the dimensions of the vector D. This is will make our computations so complex . So to avoid this problem we must use the technique  removing stop words which will remove all stop words.\n",
    "\n",
    "- Stemming : \n",
    "\n",
    "Stemming is the process of converting the words originated from same root into their base form so that we can consider all those words as one word and reduce the vector dimension D. \n",
    "\n",
    "For example Taste tasty , tastier all words are originated from same word and only difference is degrees of the word and its grammar so having multiple words for same base word will lead to increase in the dimesnions of the vector. Having them deleetd will reduce the vector dimension.\n",
    "\n",
    "\n",
    "- Lematisation:\n",
    "\n",
    "Lematisation is the process of converting the words into meaningful form. Lets New York by engilsh literature convention New york has two words however it is a single word. Lematisation will convert all words into meaningful word.\n",
    "\n",
    "\n",
    "- One of the disadvantage of the bagwords is that it considers both tasty and delicious as two words and does not consider semantic meaning. in later chapter we will study word2vec which will consider semantic meaning into consideration.\n",
    "\n",
    "r1: This pasta is very tasty and affordable \n",
    "\n",
    "r2 : This pasta is delicious and cheap.\n",
    "\n",
    "Bag of words can not consider both affordable and cheap as similar words and this will be taken care by word2vec.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41e57a3",
   "metadata": {},
   "source": [
    "## Bigrams , Trigrams and unigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c8f062",
   "metadata": {},
   "source": [
    "Lets there are two sentences \n",
    "\n",
    "r1: This pasta is very tasty and affordable.\n",
    "    \n",
    "r2 This pasta is not tasty  and affordable.\n",
    "\n",
    "Although,these two sentences are quite opposite our unigram model will create similar vectors to both after doining lemtization and tokenisation.\n",
    "\n",
    "This pasta is tasty and affordable (removed very as it is a stop word)\n",
    "\n",
    "This pasta is  tasty and affordable(removed not as it is a stop word)\n",
    "\n",
    "Now both vectors looks exactly same eventhough sentences are quite opposite. This is happening because  unigram model does not consider sequence information into account.\n",
    "\n",
    "Bigram , Trigram and n gram models give some importance to sequence infomation.\n",
    "\n",
    "(This Pasta)  (Pasta very) (very tasty_ (tasty affordable)\n",
    "\n",
    "(This Pasta) (Pasta not) (not tasty) (tasty affordable)\n",
    "\n",
    "\n",
    "In Bigrams, trigrams and n grams we captures some informations.\n",
    "\n",
    "No of words of unigram <= No of words of Bigrams\n",
    "\n",
    "- no of pairs of words will be higher than the no of words of unigram\n",
    "\n",
    " - No of trigrams will be higher than the no of words of bigram and unigram\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9211f6d",
   "metadata": {},
   "source": [
    "## TF-IDF "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234ad3c4",
   "metadata": {},
   "source": [
    "- More importantance to the raerer words in the document corpus and high  importance to the word if\n",
    "the word is repeated more times  in the review/document \n",
    "\n",
    "- TF(Term frequency)  is   \n",
    "\n",
    "## no of times a word is repeated in the document/total no of words in the document \n",
    "\n",
    "o.e probability a word appreaing in a document. Higher the value more frequent is the probability \n",
    "\n",
    "- IDF(Inverse document frquency) \n",
    "\n",
    "##  log(N/ni) \n",
    "\n",
    "- total no of documents in the corpus(All reviews)\n",
    "\n",
    "- ni is the no of times  documents contain the word wi\n",
    "\n",
    "if ni increases(more frequent word ) then N/ni decreases and log di will decrease so a more frequent word in the corpus gets less IDF value \n",
    "\n",
    "- if ni decreses then log(N/ni) increases. This means that less frequent word gets more weight(IDF)\n",
    "\n",
    "TF-IDF= TF*IDF\n",
    "\n",
    "- Count value of each word will be rep;aced by TF*IDF in bag words if TF-IDF technique is used and more common words in the corpus gets less importance and more common words in the document gets more priority "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665c0aac",
   "metadata": {},
   "source": [
    "## Why should w etake logarithm in IDF \n",
    "\n",
    "1000 documents the word \"THE\" repeats in all documents \n",
    "- Lets say the word the exists in almost all documents so n/ni is equal to 1 and log(N/ni) is also equals to 1 \n",
    "- Now lets say a word civilisation repeats in only one document  so N/ni == 1000/1  1000 when you take logarithm the value will become 7 \n",
    "\n",
    "TF*IDF --> if the words is repeated 20 times in document \n",
    "\n",
    "(20/500)* 1000 --> without log \n",
    "\n",
    "(20/500)* 7  --> if you take log\n",
    "\n",
    "so if you do not take log then IDF will completely dominate TF \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b7209b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c1cfd51",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad430d1",
   "metadata": {},
   "source": [
    "- word2vec captures the semantic meaning of the word and semantically similar words such as tasty , delicious \n",
    "- words will have same vectors.\n",
    "- The distance between the man , woman  and king , Queen will be same means the vector of (M- W) parallel ||  (K-Q)\n",
    "- The  vector distance between India , Delihi || parallel US, New york (preserves the  relationships among words)\n",
    "\n",
    "- WOrd2 vec is a d dimentional vector repr of a word but not a sparse vector \n",
    "\n",
    "- Sparse vector contains many zero values and few non zero values( Example Bag of words)\n",
    "\n",
    "- Word2vec can be any dimension such as 50,100,200,300 and higher the dimension more powerful will be the representation\n",
    "\n",
    "- it also understand tense relationship such as walk to walking vector is || to run to running\n",
    "\n",
    "- word2 vec automatically captures relationships among the words from raw text\n",
    "\n",
    "- Word2vec is trained on Google news dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b952b2",
   "metadata": {},
   "source": [
    "# How do you convert a sentence into a Word2Vec \n",
    "- There are two ways to convert a sentence into vector using word to vecotor \n",
    "\n",
    "# Avg Word2 vec\n",
    "\n",
    " w1 w2 w3 w4 w5 w6 w7 \n",
    "\n",
    "[W2v3c(W1)+W2vec(W2)+w2vec(w3)+w2vec(w4)+W2vec(W5)+w2vec(w6)+w2vec(w7)]/No of words \n",
    "\n",
    "# TfIDf word2 vec \n",
    "\n",
    "[tfidf(w1)*W2v3c(W1)+tfidf(w2)*W2vec(W2)+tfidf(w3)*w2vec(w3)+tfidf(w4)*w2vec(w4)+tfidf(w5)*W2vec(W5)+tfidf(w6)*w2vec(w6)+tfidf(w7)*w2vec(w7)]/\n",
    "(tfidf(W1)+............. tfidf(w7))\n",
    "\n",
    "- These are the two weighted schemes through which we can convert sentences into vectors using word2vec techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbfb56c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7af579",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd72ab5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
